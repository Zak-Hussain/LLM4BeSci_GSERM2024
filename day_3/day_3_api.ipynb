{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Day 3 API (bonus)\n",
    "This notebook demonstrates how to use the Hugging Face and Open AI API to classify text as neutral or partisan. The main benefit of the API is that it allows us to run the latest, largest models without having the specialised hardware needed to run them (since the models are run on the cloud). We will use the `meta-llama/Meta-Llama-3-70B-Instruct` model from Hugging Face and the `gpt-4o` model from Open AI. The downside is that the API is not (always) free."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c6844cef32967b9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Environment Setup"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4e3dda596b5a1a9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:  # If in Google Colab environment\n",
    "    # Mount google drive to enable access to data files\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Installing requisite packages\n",
    "    !pip install transformers openai  &> /dev/null\n",
    "\n",
    "    # Change working directory to day_3\n",
    "    %cd /content/drive/MyDrive/LLM4BeSci_GSERM2024/day_3"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29785427ecb9baa9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The code begins by loading some general libraries, the zero-shot prompt, and the same `media_bias_test.csv` file as in `day_3a.ipynb`. The goal will again be to classify the tweet as either `'neutral'` or `'partisan'`.  We will this time only use zero-shot classification, however the code can be easily adapted to few-shot as in `day_3a.ipynb`. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f15fd2b4f278e625"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm_notebook as tqdm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "468da181cbe06b10",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "media_bias_test = pd.read_csv('media_bias_test.csv')\n",
    "media_bias_test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26d7a352d2309870",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "zero_shot_prompt = \"Is this text neutral or partisan? Strictly answer with only 'neutral' or 'partisan':\\n\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7068281f022fdf16",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hugging Face API\n",
    "\n",
    "In order to use the Hugging Face API, you will need to follow the following steps:\n",
    "\n",
    "1.  Make sure you have a hugging Face account (https://huggingface.co/join)\n",
    "2. Go to the [Llama-3 model page](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct) and fill in the 'META LLAMA 3 COMMUNITY LICENSE AGREEMENT' form at the top of the page in order to get access to the model (this may take a few days). \n",
    "3. Once you have been granted access, you can navigate to [in your Hugging Face profile settings](https://huggingface.co/settings/tokens) to get the model's API access token. This token should provide access to all models in the Llama family. \n",
    "4. If you wish to run the largest (70B parameter) version of Llama-3, you will need to have a Hugging Face PRO subscription (currently $9/month). You can also run the smaller versions for free.\n",
    "\n",
    "The Hugging Face API code begins importing the `InferenceClient` class from the `huggingface_hub` library. The `InferenceClient` provides a high-level API to interact with models hosted on the Hugging Face Hub. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dff2d7a182ac5c61"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f32d0a87ccaaba0",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The code next initializes the `InferenceClient` with the access token, *which you will need to replace with your own access token* (access tokens start with 'hf_...') it then loops through the tweets in the `media_bias_test` dataframe just as in `day_3a.ipynb`. The code then generates the output using the `chat_completion` method of the `InferenceClient` class. This allows us to play around with certain generation-related parameters such as `max_tokens` and `temperature`. \n",
    "\n",
    "The output is then parsed to extract the label, which is then appended to the `zero_shot_labels` list. The code then adds the `zero_shot_labels` list to the `media_bias_test` dataframe."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "518e9af07bf70cce"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Initialize client\n",
    "api_key = '<your access token here>' \n",
    "client = InferenceClient(token=api_key)\n",
    "\n",
    "zero_shot_labels = []\n",
    "for tweet in tqdm(media_bias_test['text']):    \n",
    "    \n",
    "    # Zero-shot classification \n",
    "    output = client.chat_completion(\n",
    "        messages=[{\"role\": \"user\", \"content\": zero_shot_prompt + tweet}],\n",
    "        model=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n",
    "        max_tokens=100,\n",
    "        temperature=0.0\n",
    "    )\n",
    "    \n",
    "    # Accessing the text output and lowercasing it\n",
    "    output = output.choices[0].message.content.lower()\n",
    "    \n",
    "    # Extract label and append to list\n",
    "    label = 'neutral' if 'neutral' in output else 'partisan' if 'partisan' in output else 'nan' # \n",
    "    zero_shot_labels.append(label)\n",
    "\n",
    "# Add zero-shot labels to dataframe\n",
    "media_bias_test['zero_shot_label'] = zero_shot_labels\n",
    "media_bias_test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "58c0299c45039bf5",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, the accuracy of  `'Meta-Llama-3-70B-Instruct'` is roughly on par with `'Phi-3-mini-128k-instruct'` from `day_3a.ipynb`. This demonstrates that the larger model does not always outperform the smaller one:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "325a0ed5c64dd67b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Comparing zero-shot and actual labels\n",
    "print(f'Zero-shot accuracy: {(media_bias_test[\"zero_shot_label\"] == media_bias_test[\"bias\"]).mean()}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3acb73d24c9a71b0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "confusion = pd.crosstab(media_bias_test['bias'], media_bias_test['zero_shot_label'])\n",
    "sns.heatmap(confusion, annot=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f16ae4dd60624499",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**TASK 1**: Try playing around with different `temperature` values (e.g. 0.5, 1.0, and 3.0) and see how it affects the accuracy."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de549216f3999d51"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Open AI API\n",
    "We now demonstrate how to do the same as the above using Open AI API. In order to use the Open AI API, you will need to follow the following steps:\n",
    "\n",
    "1. [Create an Open AI account](https://platform.openai.com/signup)\n",
    "2. Navigate to your Profile's [billing page](https://platform.openai.com/settings/organization/billing/overview) and add some funds to your account (the minimum of $5 will be more than enough for this task).\n",
    "3. Navigate to your Profile's [API keys page](https://platform.openai.com/api-keys) and create a new API key.\n",
    "\n",
    "The code begins by importing the `OpenAI` class from the `openai` library. The `OpenAI` class provides a high-level API to interact with models hosted on the Open AI platform."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9887e5fc020df1e8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb7457a48a5f8e6e",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The code next initializes the `OpenAI` class with the API key, *which you will need to replace with your own API key*. The code then generates the output using the `chat.completions.create` method of the `OpenAI` class. This allows us to play around with certain generation-related parameters such as `max_tokens` and `temperature`."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ff1899b876a0f39"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Initialize client\n",
    "api_key = '<your api key here>'\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "zero_shot_labels = []\n",
    "for tweet in tqdm(media_bias_test['text']):\n",
    "    \n",
    "    # Zero-shot classification\n",
    "    output = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": zero_shot_prompt + tweet}],\n",
    "        model=\"gpt-4o\",\n",
    "        max_tokens=10,\n",
    "        temperature=0.0\n",
    "    )\n",
    "    \n",
    "    # Accessing the text output and lowercasing it\n",
    "    output = output.choices[0].message.content.lower()\n",
    "    \n",
    "    # Extract label and append to list\n",
    "    label = 'neutral' if 'neutral' in output else 'partisan' if 'partisan' in output else 'nan' # \n",
    "    zero_shot_labels.append(label)\n",
    "\n",
    "media_bias_test['zero_shot_label'] = zero_shot_labels\n",
    "media_bias_test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "75f50857b05a1094",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, the accuracy of  `'gpt-4o'` is considerably better than `'Meta-Llama-3-70B-Instruct'` and `'Phi-3-mini-128k-instruct'`, now at 73%:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6e1cd923ea97b60"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Comparing zero-shot and actual labels\n",
    "print(f'Zero-shot accuracy: {(media_bias_test[\"zero_shot_label\"] == media_bias_test[\"bias\"]).mean()}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af82f3c4243f2cdb",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "confusion = pd.crosstab(media_bias_test['bias'], media_bias_test['zero_shot_label'])\n",
    "sns.heatmap(confusion, annot=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c850c08ab42c7157",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**TASK 1**: Again try playing around with different `temperature` values (e.g. 0.5, 1.0, and 3.0) and see how it affects the accuracy.\n",
    "**TASK 2**: Try adapting the code from `day_3a.ipynb` to use few-shot classification instead of zero-shot classification. Does accuracy improve?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "171211cdd575c140"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
