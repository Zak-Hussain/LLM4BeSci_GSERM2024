{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Using Notebook Environments\n",
    "1. To run a cell, press `shift + enter`. The notebook will execute the code in the cell and move to the next cell. If the cell contains a markdown cell (text only), it will render the markdown and move to the next cell.\n",
    "2. Since cells can be executed in any order and variables can be over-written, you may at some point feel that you have lost track of the state of your notebook. If this is the case, you can always restart the kernel by clicking Runtime in the menu bar (if you're using Colab) and selecting `Restart runtime`. This will clear all variables and outputs.\n",
    "3. The final variable in a cell will be printed on the screen. If you want to print multiple variables, use the `print()` function as usual.\n",
    "\n",
    "Notebook environments support code cells and markdown cells. For the purposes of this workshop, markdown cells are used to provide high-level explanations of the code. More specific details are provided in the code cells themselves in the form of comments (lines beginning with `#`)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fcac050e9c92f681"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Environment Setup\n",
    "**Make sure to set your runtime to use a GPU by going to `Runtime` -> `Change runtime type` -> `Hardware accelerator` -> `T4 GPU`**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e87e9c46775bdc7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:  # If in Google Colab environment\n",
    "    # Installing requisite packages\n",
    "    !pip install transformers accelerate &> /dev/null\n",
    "\n",
    "    # Mount google drive to enable access to data files\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # Change working directory to health\n",
    "    %cd /content/drive/MyDrive/LLM4BeSci_GSERM2024/day_4"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "394aa56d7e9b7c31"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm_notebook as tqdm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2a1bee0497b7c26",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading the data\n",
    "The goal of this analysis will be to evaluate the LLM's ability to model demographic differences. We will use evaluate this using a dataset on [vaccine hesitancy](https://www.kaggle.com/datasets/christianhritter/vaccine-hesitancy-canada-cosmo-survey?select=COSMO_in_Canada_Waves_1-8_FINAL.csv). The dataset contains the following columns:\n",
    "1. `'age'`\n",
    "2. `'gender'`\n",
    "3. `'education'`\n",
    "4. `'take_vaccine'`: Survey question on willingness to take a COVID-19 vaccine (1 (Strongly Disagree) to 7 (Strongly Agree))\n",
    "5. `'mandatory_vaccine'`\n",
    "\n",
    "It also contains three '`persona`' columns, which contain the demographic information in a prompt format:\n",
    "\n",
    "1. `'persona_a'`: Gender, Age, Education\n",
    "2. `'persona_b'`: Gender, Age\n",
    "3. `'persona_c'`: Age, Education\n",
    "\n",
    "We begin by loading the dataset as a `pandas.DataFrame`:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c7eb38d7b1666f7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "vaccine = pd.read_csv('vaccine_hesitancy.csv')\n",
    "vaccine"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3bae385c289f8ff6",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can use the `value_counts()` method to get an idea of the distribution of the data. For instance, we can look at the distribution of the `'take_vaccine'` column:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9d5a19b2e72844c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "vaccine['take_vaccine'].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3f4cc9301aa4c31",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**TASK 1**: Replace `'age`' above with other demographic column names (`'gender'`, `'education'`) to get an idea of the distribution of the data.\n",
    "\n",
    "**TASK 2**: Replace `'take_vaccine'` above with other survey questions to get an idea of the distribution of the data. FYI: participants were asked the following question:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71f9095df2983fe5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the next section, we will generate responses to the survey question using the LLM. The survey question is as follows:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23c81e298cb4e644"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "survey_q = \"\"\"\n",
    "    Please give your opinion on the following statement:\n",
    "    \n",
    "    'If an effective COVID-19 vaccine becomes available and is recommended for me, I would get it.'\n",
    "    \n",
    "    Choice:\n",
    "    \n",
    "    1 = Strongly Disagree\n",
    "    2\n",
    "    3\n",
    "    4\n",
    "    5\n",
    "    6\n",
    "    7 = Strongly Agree \n",
    "    \n",
    "    Strictly only respond with the number corresponding to your choice and nothing else.\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f82d7fd82d7556a",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Comparing Phi-3's default distribution with the data\n",
    "We will generate responses to the survey question using the LLM and compare the distribution of the generated responses with the actual data. We again use microsoft's Phi-3 model. We begin by loading the model and tokenizer:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "391a9e1b4d8677b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "torch.random.manual_seed(42) # Set seed for reproducibility\n",
    "\n",
    "# Load Phi-3\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-128k-instruct\",\n",
    "    device_map=\"cuda\", # Use GPU\n",
    "    torch_dtype=torch.float16,  # Use float16 for faster inference\n",
    "    trust_remote_code=True, \n",
    "    attn_implementation='eager' # Faster inference on T4 GPUs\n",
    ")\n",
    "\n",
    "# Load tokenizer`\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fbc5cb91d927eddc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will use the `pipeline` class to generate responses to the survey question. The `generation_args` dictionary contains the following arguments:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80d3133861a77ecc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 10,  # Maximum number of tokens to generate\n",
    "    \"return_full_text\": False, # Return only the generated text\n",
    "    \"do_sample\": True, # Use greedy decoding\n",
    "    \"temperature\": 1.0  # Temperature parameter \n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e80059b11d5d40cb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will firstly generate 100 responses to the survey question and compare the distribution of the generated responses with the actual data (without giving the model any demographic information):"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ce9e99bdaf9e5a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Generate responses\n",
    "take_vaccine_preds = [] # List to store generated responses\n",
    "n_samples = 100 # Number of samples to generate\n",
    "for i in tqdm(range(n_samples)):\n",
    "    \n",
    "    # Define prompt with JSON structure\n",
    "    prompt = [{\"role\": \"user\", \"content\": survey_q}]  # Define prompt with JSON structure\n",
    "    \n",
    "    # Generate response\n",
    "    response = pipe(prompt, **generation_args)[0]['generated_text']\n",
    "    \n",
    "    # Checks which integer corresponds to the response and appends to list\n",
    "    possibles = ['1', '2', '3', '4', '5', '6', '7']\n",
    "    pred = [int(x) for x in possibles if x in response]\n",
    "    if len(pred) == 1:\n",
    "        pred = pred[0]\n",
    "        take_vaccine_preds.append(pred)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6315cfa51c6449a3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can compare the distribution of the generated responses with the actual data using a histogram:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9310e018e884f15"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Comparing distribution of generated responses with actual data\n",
    "figs, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Plotting actual data\n",
    "sns.histplot(vaccine['take_vaccine'], stat='percent', bins=7, ax=axs[0])\n",
    "axs[0].set_title('Actual Data')\n",
    "\n",
    "# Plotting generated data\n",
    "sns.histplot(take_vaccine_preds, stat='percent', bins=7, ax=axs[1])\n",
    "axs[1].set_title('Generated Data')\n",
    "axs[1].set_xlabel('take_vaccine')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7cc8691e3529b6a",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**TASK 1**: Try playing around with larger values of `temperature` (e.g. `3.0`, `5.0`, `10.0`) to see how it affects the generated responses."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10d34d37e16fde48"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Demographic Steering: Gender, Age, Education\n",
    "As mentioned, our also contains 3 \"personas\", which contain different kinds of demographic information in a prompt format:\n",
    "\n",
    "1. `'persona_a'`: Gender, Age, Education\n",
    "2. `'persona_b'`: Gender, Age\n",
    "3. `'persona_c'`: Age, Education\n",
    "\n",
    "We will now evaluate each persona's effect on the generated responses. For each \"participant\", we will append the demographic information to `survey_q` and generate a response. For instance, the prompt for the first \"participant\" would look like this:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad0307dc5fa0a47a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "prompt = f\"You are {vaccine['persona_a'][0]}.\\n-------------------------------\\n{survey_q}\"\n",
    "print(prompt)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d140d1fd835a12d",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will generate responses for each persona and compare the generated responses with the actual data. We will start with `'persona_a'` and a temperature of `3.0`, which we found to be the best temperature for this task out of [1.0, 3.0, 5.0, 10.0] using the previous task:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "474a9a09f8c91248"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "generation_args = {\n",
    "    \"max_new_tokens\": 10,  # Maximum number of tokens to generate\n",
    "    \"return_full_text\": False, # Return only the generated text\n",
    "    \"do_sample\": True, # Use greedy decoding\n",
    "    \"temperature\": 3.0  # Temperature parameter \n",
    "}\n",
    "\n",
    "\n",
    "demog_col = 'persona_a'  # Replace this for TASK 1\n",
    "\n",
    "# Generate responses\n",
    "take_vaccine_preds = [] # List to store generated responses\n",
    "for demog_prompt in tqdm(vaccine[demog_col]):\n",
    "    prompt = f\"You are {demog_prompt}.\\n-------------------------------\\n{survey_q}\"\n",
    "    \n",
    "    # Define prompt with JSON structure\n",
    "    prompt = [{\"role\": \"user\", \"content\": prompt}]  # Define prompt with JSON structure\n",
    "    \n",
    "    # Generate response\n",
    "    response = pipe(prompt, **generation_args)[0]['generated_text']\n",
    "    \n",
    "    # Checks which integer corresponds to the response\n",
    "    possibles = ['1', '2', '3', '4', '5', '6', '7']\n",
    "    pred = [int(x) for x in possibles if x in response]\n",
    "    if len(pred) == 1:\n",
    "        pred = pred[0]\n",
    "        take_vaccine_preds.append(pred)\n",
    "    else:\n",
    "        take_vaccine_preds.append(None)\n",
    "    \n",
    "# Append generated responses to dataframe\n",
    "vaccine[f'{demog_col}_preds'] = take_vaccine_preds\n",
    "vaccine[f'{demog_col}_preds'].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4108d06f5623db49"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can compare the generated responses with the actual data using a regression plot:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81282ded685f68bf"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Comparing generated and actual data with a regression plot\n",
    "ax = sns.regplot(\n",
    "    x=f'{demog_col}_preds', y='take_vaccine', x_jitter=.1, y_jitter=.1, data=vaccine,\n",
    "    scatter_kws={'alpha': 0.5}\n",
    ")\n",
    "x_y_lim = (.5, 7.5)\n",
    "ax.set(xlim=x_y_lim, ylim=x_y_lim)\n",
    "print(f\"Correlation: {vaccine['take_vaccine'].corr(vaccine[f'{demog_col}_preds'])}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44813bcb063fe68a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**TASK 1**: Try using other demographic columns (`'persona_b'`, `'persona_c'`) by replacing `'persona_a'` above to see how the generated responses change."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab785c76b21ddec"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
